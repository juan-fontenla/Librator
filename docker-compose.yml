version: '3.4'
services:
  elasticsearch:
    container_name: es01
    image: docker.elastic.co/elasticsearch/elasticsearch:8.4.3
    volumes:
      - elasticsearch:/usr/share/elasticsearch/data:z
    environment:
      ES_JAVA_OPTS: -Xms2g -Xmx2g
      bootstrap.memory_lock: "true"
      discovery.type: single-node
      xpack.security.enabled: "false"
      xpack.security.enrollment.enabled: "false"
      http.cors.enabled : "true"
      http.cors.allow-origin: http://localhost:1234
      http.cors.allow-methods: OPTIONS, HEAD, GET, POST, PUT, DELETE
      http.cors.allow-headers: X-Requested-With,X-Auth-Token,Content-Type,Content-Length,Cache-Control
      http.cors.allow-credentials: "true"
    ports:
      - 9200:9200
    networks:
      - elastic
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9200" ]
      interval: 20s
      timeout: 10s
      retries: 10

  kibana:
    image: docker.elastic.co/kibana/kibana:8.4.3
    container_name: kib01
    environment:
      XPACK_APM_SERVICEMAPENABLED: "true"
      XPACK_ENCRYPTEDSAVEDOBJECTS_ENCRYPTIONKEY: aaaaaaaa-c4d3-4a0a-8290-2abcb83ab3aa

    ports:
      - 5601:5601
    networks:
      - elastic
  
  client:
    build:
      context: ./client
      dockerfile: Dockerfile
    container_name: librator-client
    environment:
      HOST: 0.0.0.0
    ports:
      - 1234:1234
    volumes:
      - "./client/:/app/client/"

  splash:
    image: scrapinghub/splash:latest
    container_name: sp01
    ports:
      - 8050:8050
    networks:
      - elastic
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8050" ]
      interval: 20s
      timeout: 10s
      retries: 10
    
  scrapy-planetadelibros:
    build: 
      context: ./scrapper
      dockerfile: Dockerfile
      args:
        SPIDER: planetadelibros
    container_name: sc01
    depends_on:
      splash:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    networks:
      - elastic
    
  scrapy-agapea:
    build: 
      context: ./scrapper
      dockerfile: Dockerfile
      args:
        SPIDER: agapea
    container_name: sc02
    depends_on:
      splash:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    networks:
      - elastic

networks:
  elastic:

volumes:
  elasticsearch: